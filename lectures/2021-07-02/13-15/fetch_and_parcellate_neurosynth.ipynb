{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fetch_and_parcellate_neurosynth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCx8_PJhUluy"
      },
      "source": [
        "Script for fetching and parcellating Neurosynth meta-analyses (association tests) for 123 cognitive atlas terms.\n",
        "Terms will be parcellated according to the 200-node Schaefer 2018 7-network parcellation.\n",
        "This notebook will take approximately 25 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lM8Sv2Y3URa2"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Script for performing NeuroSynth-style meta-analyses for all available\n",
        "Cognitive Atlas concepts. Script modified from https://github.com/netneurolab/\n",
        "markello_spatialnulls/blob/master/scripts/empirical/fetch_neurosynth_maps.py,\n",
        "for the IPN Summer School > Quantitative and Computational Neuroscience >\n",
        "Advanced Analytics for Neuroscience > Contextualizing Results lecture.\n",
        "\n",
        "Again, this is mostly Ross' code.\n",
        "\"\"\"\n",
        "\n",
        "!pip install nilearn\n",
        "!pip install --upgrade numpy scipy matplotlib pandas\n",
        "!pip install git+https://github.com/neurosynth/neurosynth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZgYGMEU1Nr"
      },
      "source": [
        "Import stuff.\n",
        "If you run into errors, check if you need to `!pip install --upgrade` or `pip install`. \n",
        "(You shouldn't, but who knows.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3FWTlVRUVOz"
      },
      "source": [
        "import contextlib\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "from nilearn._utils import check_niimg\n",
        "\n",
        "import neurosynth as ns\n",
        "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBInV2AVIV4"
      },
      "source": [
        "Some directory set-up and warning ignoring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS3KA1CuUaPM"
      },
      "source": [
        "# /sigh\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "# this is where the raw and parcellated data will be stored\n",
        "NSDIR = Path('./data/raw/neurosynth').resolve()\n",
        "PARDIR = Path('./data/derivatives/neurosynth').resolve()\n",
        "\n",
        "# these are the images from the neurosynth analyses we'll save\n",
        "# can add 'uniformity-test_z' plus more, if desired\n",
        "IMAGES = ['association-test_z']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9e8KUXLVOU7"
      },
      "source": [
        "All the functions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ukJcuJ2UeFZ"
      },
      "source": [
        "def fetch_ns_data(directory):\n",
        "    \"\"\" Fetches NeuroSynth database + features to `directory`\n",
        "    Paramerters\n",
        "    -----------\n",
        "    directory : str or os.PathLike\n",
        "        Path to directory where data should be saved\n",
        "    Returns\n",
        "    -------\n",
        "    database, features : PathLike\n",
        "        Paths to downloaded NS data\n",
        "    \"\"\"\n",
        "\n",
        "    directory = Path(directory)\n",
        "\n",
        "    # if not already downloaded, download the NS data and unpack it\n",
        "    database, features = directory / 'database.txt', directory / 'features.txt'\n",
        "    if not database.exists() or not features.exists():\n",
        "        with open(os.devnull, 'w') as f, contextlib.redirect_stdout(f):\n",
        "            ns.dataset.download(path=directory, unpack=True)\n",
        "        try:  # remove tarball if it wasn't removed for some reason\n",
        "            (directory / 'current_data.tar.gz').unlink()\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "    return database, features\n",
        "\n",
        "\n",
        "def get_cogatlas_concepts(url=None):\n",
        "    \"\"\" Fetches list of concepts from the Cognitive Atlas\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        URL to Cognitive Atlas API\n",
        "    Returns\n",
        "    -------\n",
        "    concepts : set\n",
        "        Unordered set of terms\n",
        "    \"\"\"\n",
        "\n",
        "    if url is None:\n",
        "        url = 'https://cognitiveatlas.org/api/v-alpha/concept'\n",
        "\n",
        "    req = requests.get(url)\n",
        "    req.raise_for_status()\n",
        "    concepts = set([f.get('name') for f in json.loads(req.content)])\n",
        "\n",
        "    return concepts\n",
        "\n",
        "\n",
        "def run_meta_analyses(database, features, use_features=None, outdir=None):\n",
        "    \"\"\"\n",
        "    Runs NS-style meta-analysis based on `database` and `features`\n",
        "    Parameters\n",
        "    ----------\n",
        "    database, features : str or os.PathLike\n",
        "        Path to NS-style database.txt and features.txt files\n",
        "    use_features : list, optional\n",
        "        List of features on which to run NS meta-analyses; if not supplied all\n",
        "        terms in `features` will be used\n",
        "    outdir : str or os.PathLike\n",
        "        Path to output directory where derived files should be saved\n",
        "    Returns\n",
        "    -------\n",
        "    generated : list of str\n",
        "        List of filepaths to generated term meta-analysis directories\n",
        "    \"\"\"\n",
        "\n",
        "    # check outdir\n",
        "    if outdir is None:\n",
        "        outdir = NSDIR\n",
        "    outdir = Path(outdir)\n",
        "\n",
        "    # make database and load feature names; annoyingly slow\n",
        "    dataset = ns.Dataset(str(database))\n",
        "    dataset.add_features(str(features))\n",
        "    features = set(dataset.get_feature_names())\n",
        "\n",
        "    # if we only want a subset of the features take the set intersection\n",
        "    if use_features is not None:\n",
        "        features = set(features) & set(use_features)\n",
        "    pad = max([len(f) for f in features])\n",
        "\n",
        "    generated = []\n",
        "    for word in sorted(features):\n",
        "        msg = f'Running meta-analysis for term: {word:<{pad}}'\n",
        "        print(msg, end='\\r', flush=True)\n",
        "\n",
        "        # run meta-analysis + save specified outputs (only if they don't exist)\n",
        "        path = outdir / word.replace(' ', '_')\n",
        "        path.mkdir(exist_ok=True)\n",
        "        if not all((path / f'{f}.nii.gz').exists() for f in IMAGES):\n",
        "            ma = ns.MetaAnalysis(dataset, dataset.get_studies(features=word))\n",
        "            ma.save_results(path, image_list=IMAGES)\n",
        "\n",
        "        # store MA path\n",
        "        generated.append(path)\n",
        "\n",
        "    print(' ' * len(msg) + '\\b' * len(msg), end='', flush=True)\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def parcellate_meta(outputs, annots, fname, regions):\n",
        "    # empty dataframe to hold our parcellated data\n",
        "    data = pd.DataFrame(index=regions)\n",
        "    mask = NiftiLabelsMasker(annots, resampling_target='data')\n",
        "\n",
        "    for outdir in outputs:\n",
        "        cdata = []\n",
        "        mgh = outdir / 'association-test_z.nii.gz'\n",
        "\n",
        "        cdata.append(mask.fit_transform(\n",
        "            check_niimg(mgh.__str__(), atleast_4d=True)).squeeze())\n",
        "\n",
        "        # aaaand store it in the dataframe\n",
        "        data = data.assign(**{outdir.name: np.hstack(cdata)})\n",
        "\n",
        "    # now we save the dataframe! wooo data!\n",
        "    data.to_csv(fname, sep=',')\n",
        "    return fname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_07NHNTDVQwf"
      },
      "source": [
        "Now let's actually run the thing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dPGo3p-a4Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c400ace7-05d7-48cb-f21a-ba4bf3ad043a"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    NSDIR.mkdir(parents=True, exist_ok=True)\n",
        "    PARDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # get concepts from CogAtlas and run relevant NS meta-analysess,\n",
        "    database, features = fetch_ns_data(NSDIR)\n",
        "    generated = run_meta_analyses(database, features, get_cogatlas_concepts(),\n",
        "                                  outdir=NSDIR)\n",
        "\n",
        "    # get parcellations that we'll use to parcellate data\n",
        "    schaefer = fetch_atlas_schaefer_2018(n_rois=200, resolution_mm=2)\n",
        "    labels = []\n",
        "    for i in range(len(schaefer['labels'])):\n",
        "        labels.append(schaefer['labels'][i].decode(\"utf-8\"))\n",
        "\n",
        "    # parcellate data and save to directory\n",
        "    parcellate_meta(generated, schaefer['maps'],\n",
        "                    PARDIR / 'atl-schaefer2018_res-200_neurosynth.csv',\n",
        "                    regions=labels)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.1)\n",
            "Requirement already satisfied: nibabel>=2.5 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.2.5)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from nilearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.7.0)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->nilearn) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->nilearn) (1.15.0)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.7/dist-packages (1.21.0)\n",
            "Requirement already up-to-date: scipy in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.7/dist-packages (1.2.5)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting git+https://github.com/neurosynth/neurosynth\n",
            "  Cloning https://github.com/neurosynth/neurosynth to /tmp/pip-req-build-2422ih5v\n",
            "  Running command git clone -q https://github.com/neurosynth/neurosynth /tmp/pip-req-build-2422ih5v\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied (use --upgrade to upgrade): neurosynth==0.3.7 from git+https://github.com/neurosynth/neurosynth in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (1.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (1.7.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (1.2.5)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (3.11)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (0.22.2.post1)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (1.15.0)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.7/dist-packages (from neurosynth==0.3.7) (1.79)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurosynth==0.3.7) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->neurosynth==0.3.7) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->neurosynth==0.3.7) (1.0.1)\n",
            "Building wheels for collected packages: neurosynth\n",
            "  Building wheel for neurosynth (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neurosynth: filename=neurosynth-0.3.7-py2.py3-none-any.whl size=549000 sha256=25f7c190dc97e28d159ba3f968147fc761bf057499e9b86ea48dbe28adc1e21a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-znqryh20/wheels/db/b0/d1/4f62ab419173de3ea9d63c162183fb4319288a6bc8d762a1e7\n",
            "Successfully built neurosynth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nilearn/datasets/__init__.py:89: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
            "  \"Numpy arrays.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Dataset created in /root/nilearn_data/schaefer_2018\n",
            "\n",
            "Downloading data from https://raw.githubusercontent.com/ThomasYeoLab/CBIG/v0.14.3-Update_Yeo2011_Schaefer2018_labelname/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Schaefer2018_200Parcels_7Networks_order.txt ...\n",
            "Downloading data from https://raw.githubusercontent.com/ThomasYeoLab/CBIG/v0.14.3-Update_Yeo2011_Schaefer2018_labelname/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_2mm.nii.gz ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " ...done. (0 seconds, 0 min)\n",
            " ...done. (0 seconds, 0 min)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K26zqsmFVSwE"
      },
      "source": [
        "Download the parcellated `.csv` to upload back into your drive.\n",
        "Note you can also download the raw volumetric images from `./data/raw/neurosynth/term-of-interest/association-test_z.nii.gz`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8hNdmwc8T_Hi",
        "outputId": "2a22ed17-a913-4cc6-c9d0-62efaaae756e"
      },
      "source": [
        "files.download(PARDIR / 'atl-schaefer2018_res-200_neurosynth.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9732c9ad-4611-4fe5-80ec-4cfb25e8d525\", \"atl-schaefer2018_res-200_neurosynth.csv\", 494770)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WLqvLKaKNyy"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}